{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install llamapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, List  # Ensure this is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Qwen Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Chunk Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_chunk_datasets(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all JSON datasets and chunk data for embedding and clustering.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file), 'r') as f:\n",
    "                content = json.load(f)\n",
    "                name = content.get(\"name\", \"\")\n",
    "                uses = content.get(\"uses\", \"\")\n",
    "                side_effects = content.get(\"side_effects\", \"\")\n",
    "                dosage = content.get(\"dosage\", \"\")\n",
    "                \n",
    "                # Chunk into smaller pieces\n",
    "                chunks = [\n",
    "                    {\"text\": f\"Name: {name}. Uses: {uses}\", \"source\": file},\n",
    "                    {\"text\": f\"Side Effects: {side_effects}\", \"source\": file},\n",
    "                    {\"text\": f\"Dosage: {dosage}\", \"source\": file},\n",
    "                ]\n",
    "                data.extend(chunks)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and chunk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = '/Github_LLM/microlabs_usa'            //your dataset path\n",
    "pharma_data = load_and_chunk_datasets(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "pharma_data['embedding'] = pharma_data['text'].apply(lambda x: bert_model.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = list(pharma_data['embedding'])\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')\n",
    "pharma_data['cluster'] = clusterer.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Calculate Cluster Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cluster_centroids = (\n",
    "    pharma_data.groupby('cluster')['embedding']\n",
    "    .apply(lambda x: sum(x) / len(x))\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fit NearestNeighbors for retrieval within clusters\n",
    "clustered_data = pharma_data[pharma_data['cluster'] != -1]  # Exclude noise\n",
    "nn_models = {}\n",
    "for cluster_id in clustered_data['cluster'].unique():\n",
    "    cluster_subset = clustered_data[clustered_data['cluster'] == cluster_id]\n",
    "    nn = NearestNeighbors(n_neighbors=5, metric='euclidean')\n",
    "    nn.fit(list(cluster_subset['embedding']))\n",
    "    nn_models[cluster_id] = (nn, cluster_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: RAG-Powered Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PharmaKnowledgeAssistant:\n",
    "    def __init__(self, data: pd.DataFrame, nn_models: Dict, bert_model, cluster_centroids, tokenizer, model):\n",
    "        self.data = data\n",
    "        self.nn_models = nn_models\n",
    "        self.bert_model = bert_model\n",
    "        self.cluster_centroids = cluster_centroids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks using BERT embeddings and nearest neighbor search.\n",
    "        \"\"\"\n",
    "        query_embedding = self.bert_model.encode(query)\n",
    "\n",
    "        # Find the nearest cluster centroid\n",
    "        cluster_distances = {\n",
    "            cluster_id: sum((query_embedding - centroid) ** 2)\n",
    "            for cluster_id, centroid in self.cluster_centroids.items()\n",
    "        }\n",
    "        best_cluster = min(cluster_distances, key=cluster_distances.get)\n",
    "\n",
    "        # Retrieve nearest neighbors within the best cluster\n",
    "        nn, cluster_subset = self.nn_models[best_cluster]\n",
    "        distances, indices = nn.kneighbors([query_embedding])\n",
    "        relevant_chunks = cluster_subset.iloc[indices[0]].to_dict(orient='records')\n",
    "        return relevant_chunks\n",
    "\n",
    "    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using the Qwen 2.5-1.5B-Instruct model, augmented with retrieved context.\n",
    "        \"\"\"\n",
    "        context_text = \"\\n\".join([chunk['text'] for chunk in context_chunks])\n",
    "        prompt = (\n",
    "            f\"Context:\\n{context_text}\\n\\n\"\n",
    "            f\"User Query:\\n{query}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "            outputs = self.model.generate(inputs.input_ids, max_length=200, num_return_sequences=1)\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def recommend_safe_medication(self, symptom: str, condition: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate medication recommendations based on symptoms and conditions dynamically.\n",
    "        \"\"\"\n",
    "        # Filter the dataset for medications relevant to the symptom and condition\n",
    "        relevant_data = self.data[\n",
    "            self.data['text'].str.contains(symptom, case=False) & \n",
    "            self.data['text'].str.contains(condition, case=False)\n",
    "        ]\n",
    "        \n",
    "        if relevant_data.empty:\n",
    "            return \"No suitable medication found for the given symptoms and conditions.\"\n",
    "\n",
    "        # If relevant data is found, generate a list of medications\n",
    "        recommended_medications = []\n",
    "        for _, row in relevant_data.iterrows():\n",
    "            text = row['text']\n",
    "            source = row['source']\n",
    "            recommended_medications.append(f\"Product from {source}: {text}\")\n",
    "\n",
    "        return \"\\n\".join(recommended_medications)\n",
    "\n",
    "    def answer_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Answer the user's query using RAG or external generation.\n",
    "        \"\"\"\n",
    "        if \"symptom:\" in query.lower() and \"condition:\" in query.lower():\n",
    "            # Handle symptom-condition recommendation\n",
    "            parts = query.split(\"condition:\")\n",
    "            symptom = parts[0].replace(\"symptom:\", \"\").strip()\n",
    "            condition = parts[1].strip()\n",
    "            return self.recommend_safe_medication(symptom, condition)\n",
    "        \n",
    "        # Retrieve relevant chunks for general queries\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query)\n",
    "        return self.generate_response(query, relevant_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Interactive Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interactive_assistant():\n",
    "    \"\"\"\n",
    "    Main interactive loop for the assistant.\n",
    "    \"\"\"\n",
    "    assistant = PharmaKnowledgeAssistant(pharma_data, nn_models, bert_model, cluster_centroids, tokenizer, model)\n",
    "    print(\"Welcome to the Pharma Knowledge Assistant! Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Your Query: \")\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = assistant.answer_query(query)\n",
    "        print(\"\\nAssistant Response:\")\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run the assistant\n",
    "interactive_assistant()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
